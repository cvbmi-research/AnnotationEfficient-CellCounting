{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy.io as io\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter \n",
    "import scipy\n",
    "import json\n",
    "import torchvision.transforms.functional as F\n",
    "from matplotlib import cm as CM\n",
    "from VGG_image import * #2\n",
    "from VGG_model import * #1\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import spatial\n",
    "import operator\n",
    "cmap = plt.cm.jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' # set the GPU id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subf = 'iter1'\n",
    "tgtf = 'iter2'\n",
    "iter_k = 1 #set for select the new added samples\n",
    "\n",
    "#K = 20 # num for uncertainty\n",
    "#k = 10 # num for representativeness\n",
    "\n",
    "root = 'YOUR/DIRECTORY/OF/ACTIVE/SELECTION/FOLDER'\n",
    "dMap_folder = './VGG_dmaps_' + subf \n",
    "res_file = './VGG_results_' + subf + '.txt'\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing set\n",
    "adi_test_path = os.path.join(root,'test','images')\n",
    "test_path_sets = [adi_test_path]\n",
    "\n",
    "#validation set\n",
    "sub_path = 'val'\n",
    "adi_val_path = os.path.join(root, sub_path, 'images')\n",
    "val_path_sets = [adi_val_path]\n",
    "\n",
    "#unlabeled seting\n",
    "unlabeled_path = 'train_unlabeled_' + subf\n",
    "adi_unlabeled = os.path.join(root, unlabeled_path, 'images')\n",
    "unlabeled_sets = [adi_unlabeled]\n",
    "\n",
    "test_paths = []\n",
    "for path in test_path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.png')):\n",
    "        test_paths.append(img_path)\n",
    "\n",
    "val_paths = []\n",
    "for path in val_path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.png')):\n",
    "        val_paths.append(img_path)\n",
    "\n",
    "unlabeled_train_paths = []\n",
    "for path in unlabeled_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.png')):\n",
    "        unlabeled_train_paths.append(img_path)        \n",
    "        \n",
    "\n",
    "model1 = Generator()\n",
    "model2 = Generator()\n",
    "model3 = Generator()\n",
    "\n",
    "model1 = model1.cuda()\n",
    "model2 = model2.cuda()\n",
    "model3 = model3.cuda()\n",
    "\n",
    "saved_ckp1 = './VGG_sa_n10_m1_model_best.pth.tar' #choose the best\n",
    "saved_ckp2 = './VGG_sa_n10_m2_model_best.pth.tar' #choose the best\n",
    "saved_ckp3 = './VGG_sa_n10_m3_model_best.pth.tar' #choose the best\n",
    "\n",
    "checkpoint1 = torch.load(saved_ckp1)\n",
    "checkpoint2 = torch.load(saved_ckp2)\n",
    "checkpoint3 = torch.load(saved_ckp3)\n",
    "\n",
    "model1.load_state_dict(checkpoint1['g_state_dict'])\n",
    "model2.load_state_dict(checkpoint2['g_state_dict'])\n",
    "model3.load_state_dict(checkpoint3['g_state_dict'])\n",
    "\n",
    "loader = transforms.Compose([transforms.ToTensor()])  \n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "\n",
    "if not os.path.isdir(dMap_folder):\n",
    "    os.mkdir(dMap_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(unlabeled_train_paths), len(test_paths) # of testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae1 = 0.0\n",
    "mse1 = 0.0\n",
    "mae2 = 0.0\n",
    "mse2 = 0.0\n",
    "\n",
    "for i in range(len(test_paths)):\n",
    "    file_path = test_paths[i]\n",
    "    file_name = file_path.split('/')[-1].split('.')[0]\n",
    "    image_name1 = os.path.join(dMap_folder, file_name + '_m1_dmap.png')\n",
    "    image_name2 = os.path.join(dMap_folder, file_name + '_m2_dmap.png')\n",
    "    image_name3 = os.path.join(dMap_folder, file_name + '_m3_dmap.png')\n",
    "\n",
    "    img = transform(Image.open(test_paths[i]).convert('RGB')).cuda()\n",
    "    gt_file = h5py.File(test_paths[i].replace('.png','.h5').replace('images','ground_truth'),'r')\n",
    "    groundtruth = np.asarray(gt_file['density'])\n",
    "    \n",
    "    output1, _ = model1(img.unsqueeze(0))\n",
    "    output2, _ = model2(img.unsqueeze(0))\n",
    "    output3, _ = model3(img.unsqueeze(0))\n",
    "   \n",
    "    output_re1 = output1.view(256,-1)\n",
    "    output_re2 = output2.view(256,-1)\n",
    "    output_re3 = output3.view(256,-1)\n",
    "    \n",
    "    image1 = output1.cpu().clone()\n",
    "    image1 = image1.squeeze(0)\n",
    "    img1 = transforms.ToPILImage()(image1) # use to calculate the variance\n",
    "\n",
    "    image2 = output2.cpu().clone()\n",
    "    image2 = image2.squeeze(0)\n",
    "    img2 = transforms.ToPILImage()(image2)\n",
    "    \n",
    "    image3 = output3.cpu().clone()\n",
    "    image3 = image3.squeeze(0)\n",
    "    img3 = transforms.ToPILImage()(image3) \n",
    "    \n",
    "    k1 = output_re1.cpu().clone().detach().numpy()\n",
    "    k2 = output_re2.cpu().clone().detach().numpy()\n",
    "    k3 = output_re3.cpu().clone().detach().numpy()\n",
    "\n",
    "    plt.imsave(image_name1, k1, cmap=cmap) \n",
    "    plt.imsave(image_name2, k2, cmap=cmap) \n",
    "    plt.imsave(image_name3, k3, cmap=cmap)\n",
    "    \n",
    "    out_num1 = output1.data.cpu().clone().detach().numpy().sum()\n",
    "    out_num2 = output2.data.cpu().clone().detach().numpy().sum() \n",
    "    out_num3 = output3.data.cpu().clone().detach().numpy().sum() \n",
    "\n",
    "    gt_num = np.around(np.sum(groundtruth))\n",
    "   \n",
    "    # mean average value\n",
    "    mean_diff1 = (out_num1 + out_num2 + out_num3) / 3.0\n",
    "    mae_val1 = abs(mean_diff1 - gt_num)\n",
    "    mse_val1 = abs(mean_diff1 - gt_num)*abs(mean_diff1 - gt_num)\n",
    "    \n",
    "    # avg.\n",
    "    mae1 += mae_val1\n",
    "    mse1 += mse_val1\n",
    "\n",
    "    \n",
    "    with open(res_file, 'a+') as f:\n",
    "        out_name = file_name\n",
    "        f.write(\"{}: image: {}, MAE: {m_val1:.3f}, p1: {out1:.3f}, p2: {out2:.3f}, p3: {out3:.3f}, p_avg:{out_p:.3f}, GT:{gt:.3f}\".format(i+1, file_name, m_val1=mae_val1, out1=out_num1, out2=out_num2, out3=out_num3, out_p=mean_diff1, gt=gt_num))\n",
    "        f.write('\\n')\n",
    "    print (\"{}: image: {}, MAE: {m_val1:.3f}, p1: {out1:.3f}, p2: {out2:.3f}, p3: {out3:.3f}, p_avg:{out_p:.3f}, GT:{gt:.3f}\".format(i+1, file_name, m_val1=mae_val1, out1=out_num1, out2=out_num2, out3=out_num3, out_p=mean_diff1, gt=gt_num))\n",
    "print ('TestingSet Average MAE_<avg> error: ', mae1/len(test_paths)) \n",
    "print ('TestingSet Average MSE_<avg> error: ', np.sqrt(mse1/len(test_paths)))\n",
    "print ('********************************************')\n",
    "print ('TestingSet Average MAE_<best> error: ', mae2/len(test_paths)) \n",
    "print ('TestingSet Average MSE_<best> error: ', np.sqrt(mse2/len(test_paths)))\n",
    "\n",
    "with open(res_file, 'a+') as f:\n",
    "    f.write('TestingSet Average MAE_<avg> error: {} \\n'.format(mae1/len(test_paths)))\n",
    "    f.write('TestingSet Average MSE_<avg> error: {} \\n'.format(np.sqrt(mse1/len(test_paths))))\n",
    "    f.write('TestingSet Average MAE_<best> error: {} \\n'.format(mae2/len(test_paths)))\n",
    "    f.write('TestingSet Average MSE_<best> error: {} \\n'.format(np.sqrt(mse2/len(test_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uncertainty score of each image, method-1\n",
    "scores = {}\n",
    "clusters = {}\n",
    "keys = []\n",
    "\n",
    "for i in range(len(unlabeled_train_paths)):\n",
    "    file_path = unlabeled_train_paths[i]\n",
    "    file_name = file_path.split('/')[-1].split('.')[0]\n",
    "    img = transform(Image.open(unlabeled_train_paths[i]).convert('RGB')).cuda()\n",
    "    \n",
    "    output1, cluster1 = model1(img.unsqueeze(0))\n",
    "    output2, cluster2 = model2(img.unsqueeze(0))\n",
    "    output3, cluster3 = model3(img.unsqueeze(0))\n",
    "\n",
    "    est1 = output1.cpu().clone().detach().numpy().sum()\n",
    "    est2 = output2.cpu().clone().detach().numpy().sum()\n",
    "    est3 = output3.cpu().clone().detach().numpy().sum()\n",
    "    \n",
    "    cluster1 = np.argmax(cluster1.cpu().clone().detach().numpy())\n",
    "    cluster2 = np.argmax(cluster2.cpu().clone().detach().numpy())\n",
    "    cluster3 = np.argmax(cluster3.cpu().clone().detach().numpy())\n",
    "    #print(\"three clusters are:\", cluster1, cluster2, cluster3)\n",
    "    ests = [est1, est2, est3]\n",
    "    scores[file_name] = np.std(ests, axis=0)\n",
    "    keys.append(file_name)\n",
    "    \n",
    "    # for clusters:\n",
    "    if cluster1 == cluster2 == cluster3:\n",
    "        clus = cluster1\n",
    "    elif cluster1 == cluster2 and cluster1 != cluster3:\n",
    "        clus = cluster1\n",
    "    elif cluster1 == cluster3 and cluster1 != cluster2:\n",
    "        clus = cluster1\n",
    "    elif cluster2 == cluster3 and cluster2 != cluster1:\n",
    "        clus = cluster2\n",
    "    else:\n",
    "        # re-assign the cluster to the new patch\n",
    "        clus = 10\n",
    "    clusters[file_name] = clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uncertainty score of each image, method-2\n",
    "scores = {}\n",
    "clusters = {}\n",
    "keys = []\n",
    "\n",
    "\n",
    "# load saved checkpoints\n",
    "model1_1 = Generator().cuda()\n",
    "model1_2 = Generator().cuda()\n",
    "model1_3 = Generator().cuda()\n",
    "\n",
    "model2_1 = Generator().cuda()\n",
    "model2_2 = Generator().cuda()\n",
    "model2_3 = Generator().cuda()\n",
    "\n",
    "model3_1 = Generator().cuda()\n",
    "model3_2 = Generator().cuda()\n",
    "model3_3 = Generator().cuda()\n",
    "\n",
    "saved_ckp1_1 = './VGG_sa_n10_m1_20_checkpoint.pth.tar' \n",
    "saved_ckp1_2 = './VGG_sa_n10_m1_40_checkpoint.pth.tar' \n",
    "saved_ckp1_3 = './VGG_sa_n10_m1_60_checkpoint.pth.tar' \n",
    "\n",
    "saved_ckp2_1 = './VGG_sa_n10_m2_20_checkpoint.pth.tar'\n",
    "saved_ckp2_2 = './VGG_sa_n10_m2_40_checkpoint.pth.tar'\n",
    "saved_ckp2_3 = './VGG_sa_n10_m2_60_checkpoint.pth.tar'\n",
    "\n",
    "saved_ckp3_1 = './VGG_sa_n10_m3_20_checkpoint.pth.tar'\n",
    "saved_ckp3_2 = './VGG_sa_n10_m3_40_checkpoint.pth.tar'\n",
    "saved_ckp3_3 = './VGG_sa_n10_m3_60_checkpoint.pth.tar'\n",
    "\n",
    "checkpoint1_1 = torch.load(saved_ckp1_1)\n",
    "checkpoint1_2 = torch.load(saved_ckp1_2)\n",
    "checkpoint1_3 = torch.load(saved_ckp1_3)\n",
    "\n",
    "checkpoint2_1 = torch.load(saved_ckp2_1)\n",
    "checkpoint2_2 = torch.load(saved_ckp2_2)\n",
    "checkpoint2_3 = torch.load(saved_ckp2_3)\n",
    "\n",
    "checkpoint3_1 = torch.load(saved_ckp3_1)\n",
    "checkpoint3_2 = torch.load(saved_ckp3_2)\n",
    "checkpoint3_3 = torch.load(saved_ckp3_3)\n",
    "\n",
    "model1_1.load_state_dict(checkpoint1_1['g_state_dict'])\n",
    "model1_2.load_state_dict(checkpoint1_2['g_state_dict'])\n",
    "model1_3.load_state_dict(checkpoint1_3['g_state_dict'])\n",
    "\n",
    "model2_1.load_state_dict(checkpoint2_1['g_state_dict'])\n",
    "model2_2.load_state_dict(checkpoint2_2['g_state_dict'])\n",
    "model2_3.load_state_dict(checkpoint2_3['g_state_dict'])\n",
    "\n",
    "model3_1.load_state_dict(checkpoint3_1['g_state_dict'])\n",
    "model3_2.load_state_dict(checkpoint3_2['g_state_dict'])\n",
    "model3_3.load_state_dict(checkpoint3_3['g_state_dict'])\n",
    "\n",
    "\n",
    "for i in range(len(unlabeled_train_paths)):\n",
    "    file_path = unlabeled_train_paths[i]\n",
    "    file_name = file_path.split('/')[-1].split('.')[0]\n",
    "    img = transform(Image.open(unlabeled_train_paths[i]).convert('RGB')).cuda()\n",
    "    \n",
    "    output1_1, cluster1_1 = model1_1(img.unsqueeze(0))\n",
    "    output1_2, cluster1_2 = model1_2(img.unsqueeze(0))\n",
    "    output1_3, cluster1_3 = model1_3(img.unsqueeze(0))\n",
    "    \n",
    "    output2_1, cluster2_1 = model2_1(img.unsqueeze(0))\n",
    "    output2_2, cluster2_2 = model2_2(img.unsqueeze(0))\n",
    "    output2_3, cluster2_3 = model2_3(img.unsqueeze(0))\n",
    "    \n",
    "    output3_1, cluster3_1 = model3_1(img.unsqueeze(0))\n",
    "    output3_2, cluster3_2 = model3_2(img.unsqueeze(0))\n",
    "    output3_3, cluster3_3 = model3_3(img.unsqueeze(0))\n",
    "\n",
    "    est1_1 = output1_1.cpu().clone().detach().numpy().sum()\n",
    "    est1_2 = output1_2.cpu().clone().detach().numpy().sum()\n",
    "    est1_3 = output1_3.cpu().clone().detach().numpy().sum()\n",
    "    \n",
    "    est2_1 = output2_1.cpu().clone().detach().numpy().sum()\n",
    "    est2_2 = output2_2.cpu().clone().detach().numpy().sum()\n",
    "    est2_3 = output2_3.cpu().clone().detach().numpy().sum()\n",
    "    \n",
    "    est3_1 = output3_1.cpu().clone().detach().numpy().sum()\n",
    "    est3_2 = output3_2.cpu().clone().detach().numpy().sum()\n",
    "    est3_3 = output3_3.cpu().clone().detach().numpy().sum()\n",
    "    \n",
    "    cluster1 = np.argmax(cluster1_3.cpu().clone().detach().numpy())\n",
    "    cluster2 = np.argmax(cluster2_3.cpu().clone().detach().numpy())\n",
    "    cluster3 = np.argmax(cluster3_3.cpu().clone().detach().numpy())\n",
    "    #print(\"three clusters are:\", cluster1, cluster2, cluster3)\n",
    "    \n",
    "    ests_1 = [est1_1, est2_1, est3_1]\n",
    "    ests_2 = [est1_2, est2_2, est3_2]\n",
    "    ests_3 = [est1_3, est2_3, est3_3]\n",
    "    \n",
    "    ests_e1 = np.std(ests_1, axis=0)\n",
    "    ests_e2 = np.std(ests_2, axis=0)\n",
    "    ests_e3 = np.std(ests_3, axis=0)\n",
    "    \n",
    "    ests = [ests_e1, ests_e2, ests_e3]\n",
    "    scores[file_name] = np.std(ests, axis=0)\n",
    "    keys.append(file_name)\n",
    "    \n",
    "    # for clusters:\n",
    "    if cluster1 == cluster2 == cluster3:\n",
    "        clus = cluster1\n",
    "    elif cluster1 == cluster2 and cluster1 != cluster3:\n",
    "        clus = cluster1\n",
    "    elif cluster1 == cluster3 and cluster1 != cluster2:\n",
    "        clus = cluster1\n",
    "    elif cluster2 == cluster3 and cluster2 != cluster1:\n",
    "        clus = cluster2\n",
    "    else:\n",
    "        # re-assign the cluster to the new patch\n",
    "        clus = 10\n",
    "    clusters[file_name] = clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "cluster_root = \"YOUR/CLUSTER/FILE/FOLDER\"\n",
    "cluster_file = \"clustering_train_labels.csv\"\n",
    "results = {}\n",
    "with open(os.path.join(cluster_root, cluster_file), newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        results[row[0]] = int(row[1])\n",
    "# get exisitng files and clusters\n",
    "old_files = list(results.keys())\n",
    "old_clusters = list(results.values())\n",
    "new_clusters = list(set(old_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all existing file and clusters\n",
    "clus_total = [[] for i in range(len(new_clusters)+1)] # 10 clusters\n",
    "for idx1 in range(10):\n",
    "    for idx2 in range(len(old_clusters)):\n",
    "        if old_clusters[idx2] == idx1:\n",
    "            clus_total[idx1].append(old_files[idx2])\n",
    "existing_list = clus_total[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(existing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for new validation set & labeled train data\n",
    "new_files = list(clusters.keys())\n",
    "new_clusters = list(clusters.values())\n",
    "for idx in range(len(clusters.keys())):\n",
    "    idx_clus = new_clusters[idx]\n",
    "    clus_total[idx_clus].append(new_files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, compute the mean value vector (cetroid) for first 10 clusters\n",
    "vgg_root = \"/YOUR/DATASET/DIRECTORY\"\n",
    "\n",
    "centroids = [] # length is 10\n",
    "for i in range(10):\n",
    "    patch_fea = [] \n",
    "    for idx in range(len(clus_total[i])):\n",
    "        fea_temp = []\n",
    "        file_name = clus_total[i][idx] + \".png\"\n",
    "        patch_img = os.path.join(vgg_root, file_name)\n",
    "        img = transform(Image.open(patch_img).convert('RGB')).cuda()\n",
    "\n",
    "        model1.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output1 = model1(img.unsqueeze(0))\n",
    "        fea1 = activation['backend1']\n",
    "        fea1 = fea1.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding1 = np.mean(fea1, axis=0).flatten()\n",
    "\n",
    "        model2.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output2 = model2(img.unsqueeze(0))\n",
    "        fea2 = activation['backend1']\n",
    "        fea2 = fea2.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding2 = np.mean(fea2, axis=0).flatten()\n",
    "\n",
    "        model3.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output3 = model3(img.unsqueeze(0))\n",
    "        fea3 = activation['backend1']\n",
    "        fea3 = fea3.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding3 = np.mean(fea3, axis=0).flatten()\n",
    "\n",
    "        fea_temp = [encoding1, encoding2, encoding3]\n",
    "        patch_fea.append(np.mean(fea_temp, axis=0)) # get average feature of one patch\n",
    "    centroids.append(np.mean(patch_fea, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(clus_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only copy the new insert unlabeled data\n",
    "clus_total_copy = clus_total[:-1]\n",
    "for idx in range(len(clus_total_copy)):\n",
    "    clus_total_copy[idx] = clus_total[idx][iter_k:]\n",
    "    print(len(clus_total_copy[idx]), len(clus_total[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_total[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undecidable samples\n",
    "from scipy.spatial import distance\n",
    "for idx in range(len(clus_total[10])):\n",
    "   \n",
    "    file_name = clus_total[10][idx] + \".png\"\n",
    "    patch_img = os.path.join(vgg_root, file_name)\n",
    "    img = transform(Image.open(patch_img).convert('RGB')).cuda()\n",
    "    \n",
    "    model1.backend1.register_forward_hook(get_activation('backend1'))\n",
    "    output1 = model1(img.unsqueeze(0))\n",
    "    fea1 = activation['backend1']\n",
    "    fea1 = fea1.squeeze(0).cpu().clone().detach().numpy()\n",
    "    encoding1 = np.mean(fea1, axis=0).flatten()\n",
    "\n",
    "    model2.backend1.register_forward_hook(get_activation('backend1'))\n",
    "    output2 = model2(img.unsqueeze(0))\n",
    "    fea2 = activation['backend1']\n",
    "    fea2 = fea2.squeeze(0).cpu().clone().detach().numpy()\n",
    "    encoding2 = np.mean(fea2, axis=0).flatten()\n",
    "\n",
    "    model3.backend1.register_forward_hook(get_activation('backend1'))\n",
    "    output3 = model3(img.unsqueeze(0))\n",
    "    fea3 = activation['backend1']\n",
    "    fea3 = fea3.squeeze(0).cpu().clone().detach().numpy()\n",
    "    encoding3 = np.mean(fea3, axis=0).flatten()\n",
    "    \n",
    "    fea_temp = [encoding1, encoding2, encoding3]\n",
    "    patch_fea = np.mean(fea_temp, axis=0) # get average feature of one patch\n",
    "\n",
    "    dist_list = []\n",
    "    for idx1 in range(len(centroids)):\n",
    "        dist = np.linalg.norm(patch_fea - centroids[idx1])\n",
    "        #dist = distance.cosine(patch_fea, centroids[idx1])\n",
    "        dist_list.append(dist)\n",
    "    soft_cluster = np.argmin(dist_list) # 0-9\n",
    "    #print(dist_list)\n",
    "    #print(soft_cluster)\n",
    "    clus_total_copy[soft_cluster].append(clus_total[10][idx]) # append the patch name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check each cluster number of samples\n",
    "sum = 0\n",
    "min_val = 200000\n",
    "for idx in range(10):\n",
    "    if len(clus_total_copy[idx]) < min_val:\n",
    "        min_val = len(clus_total_copy[idx])\n",
    "    sum += len(clus_total_copy[idx])\n",
    "    print(\"the class {} has {} samples\".format(idx+1, len(clus_total_copy[idx]))) \n",
    "sum, min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = [[] for i in range(len(existing_list))]\n",
    "\n",
    "for idx in range(len(existing_list)):\n",
    "    len1 = len(existing_list[idx])\n",
    "    for idx1 in range(len1):\n",
    "        file_name = existing_list[idx][idx1] + \".png\"\n",
    "        patch_img = os.path.join(vgg_root, file_name)\n",
    "        img = transform(Image.open(patch_img).convert('RGB')).cuda()\n",
    "\n",
    "        model1.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output1 = model1(img.unsqueeze(0))\n",
    "        fea1 = activation['backend1']\n",
    "        fea1 = fea1.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding1 = np.mean(fea1, axis=0).flatten()\n",
    "\n",
    "        model2.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output2 = model2(img.unsqueeze(0))\n",
    "        fea2 = activation['backend1']\n",
    "        fea2 = fea2.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding2 = np.mean(fea2, axis=0).flatten()\n",
    "\n",
    "        model3.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output3 = model3(img.unsqueeze(0))\n",
    "        fea3 = activation['backend1']\n",
    "        fea3 = fea3.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding3 = np.mean(fea3, axis=0).flatten()\n",
    "\n",
    "        fea_temp = [encoding1, encoding2, encoding3]\n",
    "        patch_fea = np.mean(fea_temp, axis=0) # get single image patch feature\n",
    "        existing_features[idx].append(patch_fea)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each new clustered samples, compute the similarity\n",
    "sim_list = [{} for i in range(len(existing_features))]\n",
    "for idx in range(len(existing_features)):\n",
    "    for idx1 in range(len(clus_total_copy[idx])):\n",
    "        file_name = clus_total_copy[idx][idx1] + \".png\"\n",
    "        patch_img = os.path.join(vgg_root, file_name)\n",
    "        img = transform(Image.open(patch_img).convert('RGB')).cuda()\n",
    "\n",
    "        model1.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output1 = model1(img.unsqueeze(0))\n",
    "        fea1 = activation['backend1']\n",
    "        fea1 = fea1.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding1 = np.mean(fea1, axis=0).flatten()\n",
    "\n",
    "        model2.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output2 = model2(img.unsqueeze(0))\n",
    "        fea2 = activation['backend1']\n",
    "        fea2 = fea2.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding2 = np.mean(fea2, axis=0).flatten()\n",
    "\n",
    "        model3.backend1.register_forward_hook(get_activation('backend1'))\n",
    "        output3 = model3(img.unsqueeze(0))\n",
    "        fea3 = activation['backend1']\n",
    "        fea3 = fea3.squeeze(0).cpu().clone().detach().numpy()\n",
    "        encoding3 = np.mean(fea3, axis=0).flatten()\n",
    "\n",
    "        fea_temp = [encoding1, encoding2, encoding3]\n",
    "        patch_fea = np.mean(fea_temp, axis=0) # get average feature of one patch\n",
    "\n",
    "        dist = 0.0\n",
    "        for ix in range(len(existing_features[idx])):\n",
    "            dist_1 = distance.cosine(patch_fea, existing_features[idx][ix]) # new feature vs. existing features\n",
    "            dist += dist_1\n",
    "        file1 = clus_total_copy[idx][idx1]\n",
    "        sim_list[idx][file1] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_dist = sorted(sim_list[0].items(), key=lambda x: -x[1])\n",
    "sorted_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method1: considering the rarity of exisiting samples\n",
    "new_select_file = []\n",
    "new_label_list = []\n",
    "for idx in range(len(sim_list)):\n",
    "    sorted_dist = sorted(sim_list[idx].items(), key=lambda x: -x[1])\n",
    "    new_list = sorted_dist[:min_val]\n",
    "    cluster_score = {}\n",
    "    for idx1 in range(len(new_list)):\n",
    "        file_name = new_list[idx1][0]\n",
    "        cluster_score[file_name] = scores[file_name]\n",
    "    sorted_scores_list = sorted(cluster_score.items(), key=lambda x: -x[1])\n",
    "    #print(sorted_scores_list)\n",
    "    new_select_file.append(sorted_scores_list[0][0]) #file_name\n",
    "    new_label_list.append(idx)\n",
    "    print(\"Select new patch {} for cluster {}\".format(sorted_scores_list[0], idx+1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method2: direct select the first one\n",
    "new_select_file = []\n",
    "new_label_list = []\n",
    "for idx in range(len(clus_total_copy)):\n",
    "    cluster_score = {}\n",
    "    for idx1 in range(len(clus_total_copy[idx])):\n",
    "        file_name = clus_total_copy[idx][idx1]\n",
    "        cluster_score[file_name] = scores[file_name]\n",
    "    sorted_scores_list = sorted(cluster_score.items(), key=lambda x: -x[1])\n",
    "    new_select_file.append(sorted_scores_list[0][0]) #file_name\n",
    "    new_label_list.append(idx)\n",
    "    print(\"Select new patch {} for cluster {}\".format(sorted_scores_list[0], idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_select_file, new_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration-1: copy files\n",
    "from shutil import copytree\n",
    "\n",
    "src_path_train = '../YOUR/ACTIVE/SELECTION/FOLDER_SOURCE/TRAIN/' + 'train_labeled_' + subf + '/' \n",
    "tgt_path_train = '../YOUR/ACTIVE/SELECTION/FOLDER_TARGET/TRAIN/' + 'train_labeled_' + tgtf + '/' \n",
    "copytree(src_path_train, tgt_path_train, dirs_exist_ok=True)\n",
    "src_path_val = '../YOUR/ACTIVE/SELECTION/FOLDER_SOURCE/VAL/' + 'train_unlabeled_' + subf + '/' \n",
    "tgt_path_val = '../YOUR/ACTIVE/SELECTION/FOLDER_TARGET/VAL/' + 'train_unlabeled_' + tgtf + '/'\n",
    "copytree(src_path_val, tgt_path_val, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_select_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files from val to train\n",
    "from shutil import move\n",
    "for file in new_select_file:\n",
    "    src_mv_file = file + \".png\"\n",
    "    src_path1 = os.path.join(tgt_path_val + \"images\", src_mv_file) # from val folder\n",
    "    tgt_path1 = os.path.join(tgt_path_train + \"images\", src_mv_file) # to train folder\n",
    "    move(src_path1, tgt_path1)\n",
    "    tar_f1 = file + \".h5\"\n",
    "    tar_f2 = file + \".png\"\n",
    "    #tar_f3 = file + \"dots.txt\"\n",
    "    #tar_f4 = file + \"_dots.h5\"\n",
    "    #tar_f5 = file + \"cell.png\"\n",
    "    move(os.path.join(tgt_path_val + \"ground_truth\", tar_f1), os.path.join(tgt_path_train + \"ground_truth\", tar_f1))\n",
    "    move(os.path.join(tgt_path_val + \"ground_truth\", tar_f2), os.path.join(tgt_path_train + \"ground_truth\", tar_f2))\n",
    "    #move(os.path.join(tgt_path_val + \"ground_truth\", tar_f3), os.path.join(tgt_path_train + \"ground_truth\", tar_f3))\n",
    "    #move(os.path.join(tgt_path_val + \"ground_truth\", tar_f4), os.path.join(tgt_path_train + \"ground_truth\", tar_f4))\n",
    "    #move(os.path.join(tgt_path_val + \"ground_truth\", tar_f5), os.path.join(tgt_path_train + \"ground_truth\", tar_f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the selected file into CSV file\n",
    "import csv\n",
    "import os\n",
    "with open(os.path.join(cluster_root,cluster_file), \"a\") as csv_file:   \n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    level_counter = 0\n",
    "    max_levels = 10\n",
    "    while level_counter < max_levels:\n",
    "        writer.writerow((new_select_file[level_counter], new_label_list[level_counter])) \n",
    "        level_counter = level_counter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
